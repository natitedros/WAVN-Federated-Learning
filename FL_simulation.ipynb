{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "001a81a1-6009-424d-9a25-f6f81d863dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "215833ef-30da-4785-b144-357029a83817",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = './gazebo_dataset_01272026/labels_01272026.csv'\n",
    "IMG_DIR = './gazebo_dataset_01272026/images/'\n",
    "EDGE_DIR = './gazebo_dataset_01272026/edge_detection_results_01272026/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ac2382-d7e4-4f5d-91ef-b9cc1e098cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 11876\n",
      "\n",
      "First few rows:\n",
      "                               current_image  \\\n",
      "0  Loc0-102ed7ec84c44be3b4066caccff2011e.png   \n",
      "1  Loc0-102ed7ec84c44be3b4066caccff2011e.png   \n",
      "2  Loc0-102ed7ec84c44be3b4066caccff2011e.png   \n",
      "3  Loc0-102ed7ec84c44be3b4066caccff2011e.png   \n",
      "4  Loc0-102ed7ec84c44be3b4066caccff2011e.png   \n",
      "\n",
      "                           destination_image direction  \n",
      "0  Loc0-5107f16132e14cbbae95826a39aa0643.png     right  \n",
      "1  Loc0-ad13f58f6f9549b48af9a145c8398fde.png     right  \n",
      "2  Loc9-fb497088983647238d06767871bef8f7.png  backward  \n",
      "3  Loc9-057e8f35ac974ad487b4ca23310cb397.png  backward  \n",
      "4  Loc9-15467f62ac4f489194c3adb5d3fea27e.png  backward  \n",
      "\n",
      "Direction counts:\n",
      "direction\n",
      "forward     3030\n",
      "backward    3018\n",
      "left        2925\n",
      "right       2903\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nFirst few rows:\\n{df.head()}\")\n",
    "print(f\"\\nDirection counts:\\n{df['direction'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6d1b7a0-9181-43b2-9e61-9b7aebaabb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first few rows after direction mapping:\n",
      "                               current_image  \\\n",
      "0  Loc0-102ed7ec84c44be3b4066caccff2011e.png   \n",
      "1  Loc0-102ed7ec84c44be3b4066caccff2011e.png   \n",
      "2  Loc0-102ed7ec84c44be3b4066caccff2011e.png   \n",
      "3  Loc0-102ed7ec84c44be3b4066caccff2011e.png   \n",
      "4  Loc0-102ed7ec84c44be3b4066caccff2011e.png   \n",
      "\n",
      "                           destination_image direction  direction_label  \n",
      "0  Loc0-5107f16132e14cbbae95826a39aa0643.png     right                3  \n",
      "1  Loc0-ad13f58f6f9549b48af9a145c8398fde.png     right                3  \n",
      "2  Loc9-fb497088983647238d06767871bef8f7.png  backward                1  \n",
      "3  Loc9-057e8f35ac974ad487b4ca23310cb397.png  backward                1  \n",
      "4  Loc9-15467f62ac4f489194c3adb5d3fea27e.png  backward                1  \n"
     ]
    }
   ],
   "source": [
    "direction_map = {'forward': 0, 'backward': 1, 'left': 2, 'right': 3}\n",
    "df['direction_label'] = df['direction'].map(direction_map)\n",
    "num_clients = 2\n",
    "shard_size = len(df) // num_clients\n",
    "print(f\"first few rows after direction mapping:\\n{df.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb57d47-c0fc-46f7-8211-4c9117f3b090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client: 1\n",
      "Training samples: 4750\n",
      "Validation samples: 1188\n",
      "\n",
      "Client: 2\n",
      "Training samples: 4750\n",
      "Validation samples: 1188\n"
     ]
    }
   ],
   "source": [
    "# Don't shuffle - preserve original order\n",
    "# df_shuffled = df.sample(frac=1, random_state=42).reset_index(drop=True)  # REMOVED\n",
    "\n",
    "client_datasets = []\n",
    "\n",
    "for i in range(num_clients):\n",
    "    start = i * shard_size\n",
    "    end = start + shard_size\n",
    "    \n",
    "    # Get shard in original order\n",
    "    shard = df.iloc[start:end]\n",
    "    \n",
    "    # Shuffle THIS client's shard only\n",
    "    shard_shuffled = shard.sample(frac=1, random_state=42 + i).reset_index(drop=True)\n",
    "    \n",
    "    # For each shard create train/val split\n",
    "    train_df, val_df = train_test_split(shard_shuffled, test_size=0.2, random_state=42)\n",
    "    client_datasets.append((train_df, val_df))\n",
    "    \n",
    "    print(f\"\\nClient: {i+1}\")\n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e975e712-a2fc-4c90-b053-170d010c522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_cache = {}\n",
    "\n",
    "def load_image_cached(img_path):\n",
    "    if img_path not in image_cache:\n",
    "        img = load_img(img_path, target_size=(128, 128))\n",
    "        img = img_to_array(img) / 255.0\n",
    "        image_cache[img_path] = img\n",
    "    return image_cache[img_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f590aff6-c0ca-410c-8491-5f0e0109da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataframe, image_dir, batch_size=32):\n",
    "    current_images = []\n",
    "    dest_images = []\n",
    "    labels = []\n",
    "    suffix = ''\n",
    "    if image_dir == EDGE_DIR:\n",
    "        suffix = '_hed'\n",
    "        \n",
    "    for idx, row in dataframe.iterrows():\n",
    "        current_img = load_image_cached(image_dir + row['current_image'][:-4] + suffix + '.png')\n",
    "        dest_img = load_image_cached(image_dir + row['destination_image'][:-4] + suffix + '.png')\n",
    "\n",
    "        current_images.append(current_img)\n",
    "        dest_images.append(dest_img)\n",
    "        labels.append(row['direction_label'])\n",
    "\n",
    "    current_images = np.array(current_images)\n",
    "    dest_images = np.array(dest_images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return current_images, dest_images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7a36502-0aec-4d84-ac8e-d1d8773ca40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Client: 1\n",
      "Loading training data...\n",
      "Loading validation data...\n",
      "\n",
      "Training data shapes:\n",
      "Current images: (4750, 128, 128, 3)\n",
      "Destination images: (4750, 128, 128, 3)\n",
      "Labels: (4750,)\n",
      "\n",
      "Client: 2\n",
      "Loading training data...\n",
      "Loading validation data...\n",
      "\n",
      "Training data shapes:\n",
      "Current images: (4750, 128, 128, 3)\n",
      "Destination images: (4750, 128, 128, 3)\n",
      "Labels: (4750,)\n"
     ]
    }
   ],
   "source": [
    "client_train_data = []\n",
    "client_val_data = []\n",
    "\n",
    "for i, (train_df, val_df) in enumerate(client_datasets):\n",
    "\n",
    "  print(f\"\\nClient: {i+1}\")\n",
    "  print(\"Loading training data...\")\n",
    "  X_train_current, X_train_dest, y_train = create_dataset(train_df, IMG_DIR)\n",
    "\n",
    "  X_train_current_hed, X_train_dest_hed, y_train_hed = create_dataset(train_df, EDGE_DIR)\n",
    "\n",
    "\n",
    "  print(\"Loading validation data...\")\n",
    "  X_val_current, X_val_dest, y_val = create_dataset(val_df, IMG_DIR)\n",
    "\n",
    "  X_val_current_hed, X_val_dest_hed, y_val_hed = create_dataset(val_df, EDGE_DIR)\n",
    "\n",
    "\n",
    "  client_train_data.append((X_train_current, X_train_dest, X_train_current_hed, X_train_dest_hed, y_train, ))\n",
    "  client_val_data.append((X_val_current, X_val_dest, X_val_current_hed, X_val_dest_hed, y_val))\n",
    "  print(f\"\\nTraining data shapes:\")\n",
    "  print(f\"Current images: {X_train_current.shape}\")\n",
    "  print(f\"Destination images: {X_train_dest.shape}\")\n",
    "  print(f\"Labels: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fe4715d-b2c1-47e9-8ab2-8bd1d6ad33cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1\n",
      "(array([0, 1, 2, 3]), array([1236, 1217, 1168, 1129]))\n",
      "n_train: 4750\n",
      "Client 2\n",
      "(array([0, 1, 2, 3]), array([1167, 1193, 1147, 1243]))\n",
      "n_train: 4750\n",
      "Client val 1 (array([0, 1, 2, 3]), array([311, 306, 312, 259]))\n",
      "Client val 2 (array([0, 1, 2, 3]), array([316, 302, 298, 272]))\n"
     ]
    }
   ],
   "source": [
    "# for each client\n",
    "for i, (Xc, Xd, Xch, Xdh, y) in enumerate(client_train_data):\n",
    "    print(\"Client\", i+1)\n",
    "    print(np.unique(y, return_counts=True))\n",
    "    print(\"n_train:\", len(y))\n",
    "for i, (Xc, Xd, Xch, Xdh, y) in enumerate(client_val_data):\n",
    "    print(\"Client val\", i+1, np.unique(y, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cbc9e58-99a9-4f40-9c19-ed7c23a193b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_encoder(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inp)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(96, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return models.Model(inp, x, name=\"RGB_Encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b613ab8d-7a07-457e-a677-b08935a13086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hed_encoder(input_shape):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(16, 3, activation='relu', padding='same')(inp)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    return models.Model(inp, x, name=\"HED_Encoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a8b72ba-6abe-4a70-ab2c-bab8a726113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_siamese_model(H=128, W=128):\n",
    "    # Inputs\n",
    "    current_rgb = Input(shape=(H, W, 3))\n",
    "    current_hed = Input(shape=(H, W, 3))\n",
    "    dest_rgb = Input(shape=(H, W, 3))\n",
    "    dest_hed = Input(shape=(H, W, 3))\n",
    "\n",
    "    rgb_enc = rgb_encoder((H, W, 3))\n",
    "    hed_enc = hed_encoder((H, W, 3))\n",
    "\n",
    "    curr_feat = layers.Concatenate()([\n",
    "        rgb_enc(current_rgb),\n",
    "        hed_enc(current_hed)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    dest_feat = layers.Concatenate()([\n",
    "        rgb_enc(dest_rgb),\n",
    "        hed_enc(dest_hed)\n",
    "    ])\n",
    "\n",
    "    \n",
    "    diff = layers.Subtract()([dest_feat, curr_feat])\n",
    "    abs_diff = layers.Lambda(lambda x: tf.abs(x))(diff)\n",
    "\n",
    "    final_feat = layers.Concatenate()([\n",
    "        curr_feat, dest_feat, diff, abs_diff\n",
    "    ])\n",
    "\n",
    "    x = layers.Dense(256, activation='relu')(final_feat)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    output = layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(\n",
    "        inputs=[current_rgb, current_hed, dest_rgb, dest_hed],\n",
    "        outputs=output\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e533dae-f307-48af-938b-bc9e3c978999",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = build_siamese_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b6e9709-1c54-49ff-a110-01dd8d7fa668",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_models = []\n",
    "for i in range(num_clients):\n",
    "    m = build_siamese_model()\n",
    "    m.compile(optimizer='adam',\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    client_models.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5619186f-78a5-40c5-8a5e-d243d4c9741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def federated_average(models):\n",
    "    weights = [m.get_weights() for m in models]\n",
    "    new_weights = []\n",
    "\n",
    "    for layer_weights in zip(*weights):\n",
    "        new_weights.append(\n",
    "            np.mean(np.stack(layer_weights, axis=0), axis=0)\n",
    "        )\n",
    "\n",
    "    return new_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdf446a7-de47-45f3-a653-99184ef095c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Federated Round 1 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 292ms/step - accuracy: 0.2901 - loss: 1.3975 - val_accuracy: 0.3375 - val_loss: 1.3559\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 289ms/step - accuracy: 0.2514 - loss: 1.4044 - val_accuracy: 0.2753 - val_loss: 1.3887\n",
      "\n",
      "===== Federated Round 2 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 290ms/step - accuracy: 0.3105 - loss: 1.3573 - val_accuracy: 0.3636 - val_loss: 1.3189\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 285ms/step - accuracy: 0.2802 - loss: 1.3789 - val_accuracy: 0.2988 - val_loss: 1.3538\n",
      "\n",
      "===== Federated Round 3 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 290ms/step - accuracy: 0.3512 - loss: 1.3235 - val_accuracy: 0.4015 - val_loss: 1.2619\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 290ms/step - accuracy: 0.3147 - loss: 1.3563 - val_accuracy: 0.3434 - val_loss: 1.3168\n",
      "\n",
      "===== Federated Round 4 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 294ms/step - accuracy: 0.4078 - loss: 1.2447 - val_accuracy: 0.4680 - val_loss: 1.1906\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 287ms/step - accuracy: 0.3676 - loss: 1.2959 - val_accuracy: 0.3838 - val_loss: 1.2867\n",
      "\n",
      "===== Federated Round 5 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 286ms/step - accuracy: 0.4562 - loss: 1.1465 - val_accuracy: 0.4781 - val_loss: 1.1249\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 291ms/step - accuracy: 0.4341 - loss: 1.2068 - val_accuracy: 0.4545 - val_loss: 1.1541\n",
      "\n",
      "===== Federated Round 6 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 294ms/step - accuracy: 0.5023 - loss: 1.0608 - val_accuracy: 0.5008 - val_loss: 1.1142\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 296ms/step - accuracy: 0.4935 - loss: 1.0894 - val_accuracy: 0.5219 - val_loss: 1.0495\n",
      "\n",
      "===== Federated Round 7 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 293ms/step - accuracy: 0.5598 - loss: 0.9513 - val_accuracy: 0.6061 - val_loss: 0.8643\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 292ms/step - accuracy: 0.5745 - loss: 0.9543 - val_accuracy: 0.6145 - val_loss: 0.8942\n",
      "\n",
      "===== Federated Round 8 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 288ms/step - accuracy: 0.6269 - loss: 0.8362 - val_accuracy: 0.6456 - val_loss: 0.8415\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 291ms/step - accuracy: 0.6657 - loss: 0.7968 - val_accuracy: 0.6448 - val_loss: 0.8288\n",
      "\n",
      "===== Federated Round 9 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 288ms/step - accuracy: 0.6933 - loss: 0.7164 - val_accuracy: 0.6852 - val_loss: 0.7383\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 285ms/step - accuracy: 0.7166 - loss: 0.6936 - val_accuracy: 0.7020 - val_loss: 0.7452\n",
      "\n",
      "===== Federated Round 10 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 293ms/step - accuracy: 0.7272 - loss: 0.6504 - val_accuracy: 0.7214 - val_loss: 0.6819\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 289ms/step - accuracy: 0.7539 - loss: 0.6147 - val_accuracy: 0.7189 - val_loss: 0.7071\n",
      "\n",
      "===== Federated Round 11 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 287ms/step - accuracy: 0.7547 - loss: 0.5766 - val_accuracy: 0.7222 - val_loss: 0.6841\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 288ms/step - accuracy: 0.7859 - loss: 0.5404 - val_accuracy: 0.7677 - val_loss: 0.5862\n",
      "\n",
      "===== Federated Round 12 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 291ms/step - accuracy: 0.7659 - loss: 0.5448 - val_accuracy: 0.7340 - val_loss: 0.6844\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 291ms/step - accuracy: 0.7914 - loss: 0.5123 - val_accuracy: 0.7483 - val_loss: 0.5977\n",
      "\n",
      "===== Federated Round 13 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 285ms/step - accuracy: 0.7909 - loss: 0.5016 - val_accuracy: 0.7500 - val_loss: 0.5999\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 287ms/step - accuracy: 0.8126 - loss: 0.4698 - val_accuracy: 0.7416 - val_loss: 0.6047\n",
      "\n",
      "===== Federated Round 14 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 288ms/step - accuracy: 0.8131 - loss: 0.4601 - val_accuracy: 0.7753 - val_loss: 0.5975\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 286ms/step - accuracy: 0.8360 - loss: 0.4316 - val_accuracy: 0.7845 - val_loss: 0.5166\n",
      "\n",
      "===== Federated Round 15 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 290ms/step - accuracy: 0.8183 - loss: 0.4483 - val_accuracy: 0.7955 - val_loss: 0.5612\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 289ms/step - accuracy: 0.8377 - loss: 0.4184 - val_accuracy: 0.7963 - val_loss: 0.5317\n",
      "\n",
      "===== Federated Round 16 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 298ms/step - accuracy: 0.8318 - loss: 0.4138 - val_accuracy: 0.8005 - val_loss: 0.5381\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 290ms/step - accuracy: 0.8419 - loss: 0.4021 - val_accuracy: 0.8005 - val_loss: 0.5120\n",
      "\n",
      "===== Federated Round 17 =====\n",
      "Freezing encoders and recompiling models\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 235ms/step - accuracy: 0.8373 - loss: 0.4060 - val_accuracy: 0.7946 - val_loss: 0.5553\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 250ms/step - accuracy: 0.8564 - loss: 0.3703 - val_accuracy: 0.7736 - val_loss: 0.5660\n",
      "\n",
      "===== Federated Round 18 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 240ms/step - accuracy: 0.8613 - loss: 0.3551 - val_accuracy: 0.7576 - val_loss: 0.6511\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 239ms/step - accuracy: 0.8718 - loss: 0.3363 - val_accuracy: 0.7980 - val_loss: 0.4984\n",
      "\n",
      "===== Federated Round 19 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 243ms/step - accuracy: 0.8556 - loss: 0.3549 - val_accuracy: 0.7870 - val_loss: 0.5522\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 241ms/step - accuracy: 0.8676 - loss: 0.3308 - val_accuracy: 0.8131 - val_loss: 0.5189\n",
      "\n",
      "===== Federated Round 20 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 246ms/step - accuracy: 0.8665 - loss: 0.3307 - val_accuracy: 0.7938 - val_loss: 0.5294\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 236ms/step - accuracy: 0.8745 - loss: 0.3145 - val_accuracy: 0.8064 - val_loss: 0.4858\n",
      "\n",
      "===== Federated Round 21 =====\n",
      "Freezing encoders and recompiling models\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 98ms/step - accuracy: 0.9027 - loss: 0.2455 - val_accuracy: 0.8392 - val_loss: 0.4738\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 99ms/step - accuracy: 0.9135 - loss: 0.2197 - val_accuracy: 0.8316 - val_loss: 0.4516\n",
      "\n",
      "===== Federated Round 22 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 97ms/step - accuracy: 0.9097 - loss: 0.2225 - val_accuracy: 0.8426 - val_loss: 0.4804\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 96ms/step - accuracy: 0.9215 - loss: 0.1977 - val_accuracy: 0.8359 - val_loss: 0.4572\n",
      "\n",
      "===== Federated Round 23 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 100ms/step - accuracy: 0.9133 - loss: 0.2138 - val_accuracy: 0.8367 - val_loss: 0.4946\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 99ms/step - accuracy: 0.9229 - loss: 0.1903 - val_accuracy: 0.8182 - val_loss: 0.5407\n",
      "\n",
      "===== Federated Round 24 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 103ms/step - accuracy: 0.9204 - loss: 0.2029 - val_accuracy: 0.8384 - val_loss: 0.5131\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 96ms/step - accuracy: 0.9280 - loss: 0.1853 - val_accuracy: 0.8333 - val_loss: 0.4810\n",
      "\n",
      "===== Federated Round 25 =====\n",
      "Training model for client 1\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 99ms/step - accuracy: 0.9179 - loss: 0.2045 - val_accuracy: 0.8350 - val_loss: 0.5109\n",
      "Training model for client 2\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 98ms/step - accuracy: 0.9316 - loss: 0.1773 - val_accuracy: 0.8232 - val_loss: 0.4847\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 25\n",
    "local_epochs = 1\n",
    "hed_freeze_round = 16\n",
    "rgb_freeze_round = 20\n",
    "\n",
    "for round_idx in range(num_rounds):\n",
    "    \n",
    "    print(f\"\\n===== Federated Round {round_idx+1} =====\")\n",
    "\n",
    "    if round_idx == rgb_freeze_round:\n",
    "        print(\"Freezing encoders and recompiling models\")\n",
    "\n",
    "        for m in client_models:\n",
    "            m.get_layer(\"RGB_Encoder\").trainable = False\n",
    "\n",
    "            m.compile(\n",
    "                optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "        global_model.get_layer(\"RGB_Encoder\").trainable = False\n",
    "\n",
    "        global_model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "    if round_idx == hed_freeze_round:\n",
    "        print(\"Freezing encoders and recompiling models\")\n",
    "\n",
    "        for m in client_models:\n",
    "            m.get_layer(\"HED_Encoder\").trainable = False\n",
    "\n",
    "            m.compile(\n",
    "                optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "        global_model.get_layer(\"HED_Encoder\").trainable = False\n",
    "\n",
    "        global_model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "    global_weights = global_model.get_weights()\n",
    "    # Braodcast weights\n",
    "    for m in client_models:\n",
    "        m.set_weights(global_weights)\n",
    "\n",
    "    # Local training\n",
    "    for i in range(num_clients):\n",
    "        print(f\"Training model for client {i+1}\")\n",
    "        X_train_current, X_train_dest, X_train_current_hed, X_train_dest_hed, y_train = client_train_data[i]\n",
    "        X_val_current, X_val_dest, X_val_current_hed, X_val_dest_hed, y_val = client_val_data[i]\n",
    "        client_models[i].fit(\n",
    "        [\n",
    "            X_train_current,\n",
    "            X_train_current_hed,\n",
    "            X_train_dest,\n",
    "            X_train_dest_hed\n",
    "        ],\n",
    "        y_train,\n",
    "        validation_data=(\n",
    "            [\n",
    "                X_val_current,\n",
    "                X_val_current_hed,\n",
    "                X_val_dest,\n",
    "                X_val_dest_hed\n",
    "            ],\n",
    "            y_val\n",
    "        ),\n",
    "        epochs=local_epochs,\n",
    "        batch_size=16\n",
    "    )\n",
    "\n",
    "    # Aggregate\n",
    "    global_model.set_weights(\n",
    "        federated_average(client_models)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab34e2e8-6a66-4345-80d2-97a403f06819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating validation data for client 1\n",
      "Evaluating validation data for client 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for i in range(num_clients):\n",
    "    print(f\"Evaluating validation data for client {i+1}\")\n",
    "\n",
    "    X_val_current, X_val_dest, X_val_current_hed, X_val_dest_hed, y_val = client_val_data[i]\n",
    "\n",
    "    preds = global_model.predict(\n",
    "        [\n",
    "            X_val_current,\n",
    "            X_val_current_hed,\n",
    "            X_val_dest,\n",
    "            X_val_dest_hed\n",
    "        ],\n",
    "        batch_size=16,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "    y_true_all.append(y_val)\n",
    "    y_pred_all.append(y_pred)\n",
    "\n",
    "# Concatenate across all clients\n",
    "y_true_all = np.concatenate(y_true_all)\n",
    "y_pred_all = np.concatenate(y_pred_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "952d5a69-c8bb-4c02-9817-c2653d901b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Global Model Validation Metrics =====\n",
      "Accuracy : 0.8422\n",
      "Precision: 0.8422\n",
      "Recall   : 0.8405\n",
      "F1 Score : 0.8409\n",
      "\n",
      "Per-class metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       627\n",
      "           1       0.85      0.85      0.85       608\n",
      "           2       0.88      0.83      0.85       610\n",
      "           3       0.82      0.80      0.81       531\n",
      "\n",
      "    accuracy                           0.84      2376\n",
      "   macro avg       0.84      0.84      0.84      2376\n",
      "weighted avg       0.84      0.84      0.84      2376\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_true_all, y_pred_all)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_true_all,\n",
    "    y_pred_all,\n",
    "    average='macro'\n",
    ")\n",
    "\n",
    "print(\"\\n===== Global Model Validation Metrics =====\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "print(\"\\nPer-class metrics:\")\n",
    "print(classification_report(y_true_all, y_pred_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d72078-10f0-4a9f-b743-65e921575fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cnn_env)",
   "language": "python",
   "name": "cnn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
